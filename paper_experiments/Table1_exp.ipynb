{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmFUpHaTavmF"
      },
      "source": [
        "# **Reproducing Unsupervised results (Table 1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yroHRAIvavmM"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/blackswan-advitamaeternam/HVAE/blob/raph/paper_experiments/Table1_exp.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXwoVTMYavmN"
      },
      "source": [
        "## **Colab setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D5sCNkiyavmN",
        "outputId": "8299c423-fbb8-4906-c544-46284cecf2a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gDVkRAoOavmP",
        "outputId": "3f294b04-0d00-4410-9986-8aea9114c678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HVAE'...\n",
            "remote: Enumerating objects: 458, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 458 (delta 77), reused 36 (delta 19), pack-reused 317 (from 1)\u001b[K\n",
            "Receiving objects: 100% (458/458), 1.81 MiB | 5.27 MiB/s, done.\n",
            "Resolving deltas: 100% (299/299), done.\n",
            "/content/HVAE\n",
            "Branch 'raph' set up to track remote branch 'raph' from 'origin'.\n",
            "Switched to a new branch 'raph'\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.16.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.13.2)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.5.9.post2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
            "Collecting pathlib (from -r requirements.txt (line 8))\n",
            "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.24.0+cpu)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (4.0.0)\n",
            "Collecting scanpy (from -r requirements.txt (line 13))\n",
            "  Downloading scanpy-1.11.5-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn->-r requirements.txt (line 6)) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn->-r requirements.txt (line 6)) (0.5.13)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (2025.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (0.70.16)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (6.0.3)\n",
            "Collecting anndata>=0.8 (from scanpy->-r requirements.txt (line 13))\n",
            "  Downloading anndata-0.12.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: h5py>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from scanpy->-r requirements.txt (line 13)) (3.15.1)\n",
            "Collecting legacy-api-wrap>=1.4.1 (from scanpy->-r requirements.txt (line 13))\n",
            "  Downloading legacy_api_wrap-1.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from scanpy->-r requirements.txt (line 13)) (8.4.0)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scanpy->-r requirements.txt (line 13)) (1.0.2)\n",
            "Collecting session-info2 (from scanpy->-r requirements.txt (line 13))\n",
            "  Downloading session_info2-0.3-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.12/dist-packages (from scanpy->-r requirements.txt (line 13)) (0.14.6)\n",
            "Collecting array-api-compat>=1.7.1 (from anndata>=0.8->scanpy->-r requirements.txt (line 13))\n",
            "  Downloading array_api_compat-1.13.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting zarr!=3.0.*,>=2.18.7 (from anndata>=0.8->scanpy->-r requirements.txt (line 13))\n",
            "  Downloading zarr-3.1.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->-r requirements.txt (line 12)) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.2->umap-learn->-r requirements.txt (line 6)) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 12)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 12)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 12)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 12)) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 10)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 10)) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (1.22.0)\n",
            "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy->-r requirements.txt (line 13))\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy->-r requirements.txt (line 13)) (1.7.1)\n",
            "Collecting numcodecs>=0.14 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy->-r requirements.txt (line 13))\n",
            "  Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
            "Downloading scanpy-1.11.5-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.12.7-py3-none-any.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.2/174.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.5-py3-none-any.whl (10 kB)\n",
            "Downloading session_info2-0.3-py3-none-any.whl (17 kB)\n",
            "Downloading array_api_compat-1.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-3.1.5-py3-none-any.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (9.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pathlib, session-info2, numcodecs, legacy-api-wrap, donfig, array-api-compat, zarr, anndata, scanpy\n",
            "Successfully installed anndata-0.12.7 array-api-compat-1.13.0 donfig-0.8.1.post1 legacy-api-wrap-1.5 numcodecs-0.16.5 pathlib-1.0.1 scanpy-1.11.5 session-info2-0.3 zarr-3.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pathlib"
                ]
              },
              "id": "d5cc3b40b1324e3aa371e1f3affc5730"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/blackswan-advitamaeternam/HVAE.git\n",
        "%cd HVAE\n",
        "!git checkout raph\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuAzmSR9avmQ"
      },
      "source": [
        "To allow automatic reloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_jGdfarMavmR",
        "outputId": "1e10ca8a-4bbb-4c08-a404-af579ade6676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (7.34.0)\n",
            "Collecting ipython\n",
            "  Downloading ipython-9.8.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: decorator>=4.3.2 in /usr/local/lib/python3.12/dist-packages (from ipython) (4.4.2)\n",
            "Collecting ipython-pygments-lexers>=1.0.0 (from ipython)\n",
            "  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting jedi>=0.18.1 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from ipython) (0.2.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.11.0 in /usr/local/lib/python3.12/dist-packages (from ipython) (2.19.2)\n",
            "Collecting stack_data>=0.6.0 (from ipython)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting traitlets>=5.13.0 (from ipython)\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.18.1->ipython) (0.8.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.14)\n",
            "Collecting executing>=1.2.0 (from stack_data>=0.6.0->ipython)\n",
            "  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack_data>=0.6.0->ipython)\n",
            "  Downloading asttokens-3.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pure-eval (from stack_data>=0.6.0->ipython)\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Downloading ipython-9.8.0-py3-none-any.whl (621 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m621.4/621.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asttokens-3.0.1-py3-none-any.whl (27 kB)\n",
            "Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pure-eval, traitlets, jedi, ipython-pygments-lexers, executing, asttokens, stack_data, ipython\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.7.1\n",
            "    Uninstalling traitlets-5.7.1:\n",
            "      Successfully uninstalled traitlets-5.7.1\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 7.34.0\n",
            "    Uninstalling ipython-7.34.0:\n",
            "      Successfully uninstalled ipython-7.34.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 9.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asttokens-3.0.1 executing-2.2.1 ipython-9.8.0 ipython-pygments-lexers-1.1.1 jedi-0.19.2 pure-eval-0.2.3 stack_data-0.6.3 traitlets-5.14.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "traitlets"
                ]
              },
              "id": "1279138dc1ff4fe0bf542489741f885f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g23sNB6WavmR"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "try:\n",
        "    import imp\n",
        "except ImportError:\n",
        "    import types\n",
        "    sys.modules['imp'] = types.ModuleType('imp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aUiV3rkqavmS"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLPOGkKuavmS"
      },
      "source": [
        "## **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cynfn6YKavmS"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "path_to_repo = \"/content/HVAE\"\n",
        "if path_to_repo not in sys.path:\n",
        "    sys.path.append(path_to_repo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hmrXnpW2avmT",
        "outputId": "29fd6db3-10b8-4260-f81f-82a8abb479cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3199704265.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2202\u001b[0m )\n\u001b[1;32m   2203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2204\u001b[0;31m from torch import (\n\u001b[0m\u001b[1;32m   2205\u001b[0m     \u001b[0m__config__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__config__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2206\u001b[0m     \u001b[0m__future__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__future__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \"\"\"\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_verbose_message\u001b[0;34m(message, verbosity, *args)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from svae.vae import SVAE, GaussianVAE\n",
        "from svae.training import training\n",
        "from paper_experiments.load_MNIST import load_mnist, ShuffledLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV7bTh7_avmT"
      },
      "source": [
        "Setting device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJW-1lU3avmU"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzKTS2BHavmU"
      },
      "outputs": [],
      "source": [
        "NUM_WORKERS = int(0.8*os.cpu_count())\n",
        "FRAC = 1.0\n",
        "TRAIN_FRAC = int(50000 * FRAC)\n",
        "VAL_FRAC = int(10000 * FRAC)\n",
        "TEST_FRAC = None\n",
        "print(f\"Using {TRAIN_FRAC} train samples, {VAL_FRAC} val samples\")\n",
        "\n",
        "train_loader, val_loader, test_loader = load_mnist(\n",
        "    train_size=TRAIN_FRAC,\n",
        "    val_size=VAL_FRAC,\n",
        "    test_size=TEST_FRAC,\n",
        "    batch_size=64,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    binarize=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeHjPtpFavmU"
      },
      "outputs": [],
      "source": [
        "train_batches = [[el.to(DEVICE) for el in batch] for batch in train_loader]\n",
        "val_batches   = [[el.to(DEVICE) for el in batch] for batch in val_loader]\n",
        "test_batches  = [[el.to(DEVICE) for el in batch] for batch in test_loader]\n",
        "\n",
        "train_loader = ShuffledLoader(train_batches, shuffle_batches=True, shuffle_within=True)\n",
        "val_loader = ShuffledLoader(val_batches, shuffle_batches=False, shuffle_within=False)\n",
        "test_loader = ShuffledLoader(test_batches, shuffle_batches=False, shuffle_within=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKyp40TlavmV"
      },
      "source": [
        "## **Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usGOrdi9avmV"
      },
      "outputs": [],
      "source": [
        "base_path = \"/content/drive/MyDrive/HVAE/Table1_results/\"\n",
        "os.makedirs(base_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpH0kk5RavmV"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 500\n",
        "INPUT_DIM = 784\n",
        "HIDDEN_DIM = 128\n",
        "\n",
        "PATIENCE = 50\n",
        "WARMUP = 100\n",
        "ONE_LAYER = False\n",
        "LR = 1e-3\n",
        "BETA_KL = 1.0\n",
        "\n",
        "N_RUNS = 10\n",
        "N_LL_SAMPLES = 500\n",
        "\n",
        "LATENT_DIMS = [2, 5, 10, 20, 40]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rehdB9H3avmV"
      },
      "source": [
        "## **Evaluation Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rkf5eAy5avmV"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(model, test_tensor, N_ll_samples=500):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        LL = model.total_marginal_ll(test_tensor, N=N_ll_samples, reduced='mean').item()\n",
        "\n",
        "        _, parts = model.full_step(test_tensor, beta_kl=1.0)\n",
        "        RE = -parts['recon'].item()\n",
        "        KL = parts['kl'].item()\n",
        "\n",
        "        ELBO = RE - KL\n",
        "\n",
        "    return {\n",
        "        'LL': LL,\n",
        "        'ELBO': ELBO,\n",
        "        'RE': RE,\n",
        "        'KL': KL\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZ7nQvLFavmW"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(mode, latent_dim, train_loader, val_loader, test_tensor, n_ll_samples):\n",
        "\n",
        "    addon = \"[SVAE]\" if mode == \"svae\" else \"[NVAE]\"\n",
        "    print(f\"\\n{addon} Training with latent_dim={latent_dim}..\")\n",
        "\n",
        "    if mode == \"svae\":\n",
        "        model = SVAE(\n",
        "            input_dim=INPUT_DIM,\n",
        "            hidden_dim=HIDDEN_DIM,\n",
        "            latent_dim=latent_dim,\n",
        "            one_layer=ONE_LAYER,\n",
        "            mode='mnist'\n",
        "        )\n",
        "    else:\n",
        "        model = GaussianVAE(\n",
        "            input_dim=INPUT_DIM,\n",
        "            hidden_dim=HIDDEN_DIM,\n",
        "            latent_dim=latent_dim,\n",
        "            one_layer=ONE_LAYER,\n",
        "            mode='mnist'\n",
        "        )\n",
        "\n",
        "    model.to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    model, losses, all_parts = training(\n",
        "        dataloader=train_loader,\n",
        "        val_dataloader=val_loader,\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        epochs=EPOCHS,\n",
        "        beta_kl=BETA_KL,\n",
        "        warmup=WARMUP,\n",
        "        patience=PATIENCE,\n",
        "        show_loss_every=50\n",
        "    )\n",
        "\n",
        "    print(f\"{addon} Computing metrics with {n_ll_samples} importance samples..\")\n",
        "    metrics = compute_metrics(model, test_tensor, N_ll_samples=n_ll_samples)\n",
        "\n",
        "    print(f\"{addon} LL={metrics['LL']:.2f}, ELBO={metrics['ELBO']:.2f}, RE={metrics['RE']:.2f}, KL={metrics['KL']:.2f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1sbRD9eavmW"
      },
      "outputs": [],
      "source": [
        "def run_table1_experiment(latent_dims, n_runs, n_ll_samples, train_loader, val_loader, test_loader):\n",
        "\n",
        "    test_data = [batch[0] for batch in test_loader]\n",
        "    test_tensor = torch.cat(test_data, dim=0).to(DEVICE)\n",
        "    print(f\"Test tensor shape: {test_tensor.shape}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for latent_dim in tqdm(latent_dims, desc=\"Latent dimensions\"):\n",
        "        for mode in [\"normal\", \"svae\"]:\n",
        "            model_name = \"S-VAE\" if mode == \"svae\" else \"N-VAE\"\n",
        "\n",
        "            run_metrics = {'LL': [], 'ELBO': [], 'RE': [], 'KL': []}\n",
        "\n",
        "            for run in tqdm(range(n_runs), desc=f\"{model_name} d={latent_dim}\", leave=False):\n",
        "                print(f\"\\n{'='*50}\")\n",
        "                print(f\"RUN {run+1}/{n_runs} | {model_name} | d={latent_dim}\")\n",
        "                print('='*50)\n",
        "\n",
        "                metrics = train_and_evaluate(\n",
        "                    mode=mode,\n",
        "                    latent_dim=latent_dim,\n",
        "                    train_loader=train_loader,\n",
        "                    val_loader=val_loader,\n",
        "                    test_tensor=test_tensor,\n",
        "                    n_ll_samples=n_ll_samples\n",
        "                )\n",
        "\n",
        "                for key in run_metrics:\n",
        "                    run_metrics[key].append(metrics[key])\n",
        "\n",
        "            results.append({\n",
        "                'Method': model_name,\n",
        "                'd': latent_dim,\n",
        "                'LL': f\"{np.mean(run_metrics['LL']):.2f}±{np.std(run_metrics['LL']):.2f}\",\n",
        "                'L[q]': f\"{np.mean(run_metrics['ELBO']):.2f}±{np.std(run_metrics['ELBO']):.2f}\",\n",
        "                'RE': f\"{np.mean(run_metrics['RE']):.2f}±{np.std(run_metrics['RE']):.2f}\",\n",
        "                'KL': f\"{np.mean(run_metrics['KL']):.2f}±{np.std(run_metrics['KL']):.2f}\",\n",
        "\n",
        "                'LL_mean': np.mean(run_metrics['LL']),\n",
        "                'LL_std': np.std(run_metrics['LL']),\n",
        "                'ELBO_mean': np.mean(run_metrics['ELBO']),\n",
        "                'ELBO_std': np.std(run_metrics['ELBO']),\n",
        "                'RE_mean': np.mean(run_metrics['RE']),\n",
        "                'RE_std': np.std(run_metrics['RE']),\n",
        "                'KL_mean': np.mean(run_metrics['KL']),\n",
        "                'KL_std': np.std(run_metrics['KL'])\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyudVz2HavmW"
      },
      "source": [
        "## **Run Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnBnJ9uHavmX"
      },
      "outputs": [],
      "source": [
        "results_df = run_table1_experiment(\n",
        "    latent_dims=LATENT_DIMS,\n",
        "    n_runs=N_RUNS,\n",
        "    n_ll_samples=N_LL_SAMPLES,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhsxQbE8avmX"
      },
      "outputs": [],
      "source": [
        "results_df.to_csv(base_path + \"Table1_results.csv\", index=False)\n",
        "print(f\"Results saved to {base_path}Table1_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qeshLUhavmX"
      },
      "source": [
        "## **Display Results (Table 1 Format)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Heckb4EeavmX"
      },
      "outputs": [],
      "source": [
        "display_df = results_df[['Method', 'd', 'LL', 'L[q]', 'RE', 'KL']].copy()\n",
        "display_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9DfMLjLavmX"
      },
      "outputs": [],
      "source": [
        "def format_table1_paper_style(df):\n",
        "\n",
        "    nvae = df[df['Method'] == 'N-VAE'].set_index('d')\n",
        "    svae = df[df['Method'] == 'S-VAE'].set_index('d')\n",
        "\n",
        "    print(\"=\"*100)\n",
        "    print(\"TABLE 1: Summary of results (mean and standard-deviation over runs) of unsupervised model on MNIST\")\n",
        "    print(\"=\"*100)\n",
        "    print(f\"{'Method':<10} | {'LL':^20} | {'L[q]':^20} | {'RE':^20} | {'KL':^15}\")\n",
        "    print(\"-\"*100)\n",
        "\n",
        "    for d in LATENT_DIMS:\n",
        "\n",
        "        print(f\"{'N-VAE':<7} d={d:<2} | {nvae.loc[d, 'LL']:^20} | {nvae.loc[d, 'L[q]']:^20} | {nvae.loc[d, 'RE']:^20} | {nvae.loc[d, 'KL']:^15}\")\n",
        "\n",
        "        print(f\"{'S-VAE':<7} d={d:<2} | {svae.loc[d, 'LL']:^20} | {svae.loc[d, 'L[q]']:^20} | {svae.loc[d, 'RE']:^20} | {svae.loc[d, 'KL']:^15}\")\n",
        "        print(\"-\"*100)\n",
        "\n",
        "format_table1_paper_style(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LY1L5d21avmX"
      },
      "source": [
        "## **Generate LaTeX Table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFxTOqAqavmX"
      },
      "outputs": [],
      "source": [
        "def generate_latex_table(df):\n",
        "\n",
        "    latex = r\"\"\"\n",
        "\\begin{table}[h]\n",
        "\\centering\n",
        "\\caption{Summary of results (mean and standard-deviation over \"\"\" + str(N_RUNS) + r\"\"\" runs) of unsupervised model on MNIST.}\n",
        "\\begin{tabular}{ll|cccc}\n",
        "\\toprule\n",
        "Method & $d$ & LL & $\\mathcal{L}[q]$ & RE & KL \\\\\n",
        "\\midrule\n",
        "\"\"\"\n",
        "\n",
        "    for d in LATENT_DIMS:\n",
        "        for method in ['N-VAE', 'S-VAE']:\n",
        "            row = df[(df['Method'] == method) & (df['d'] == d)].iloc[0]\n",
        "            prefix = r\"$\\mathcal{N}$\" if method == 'N-VAE' else r\"$\\mathcal{S}$\"\n",
        "\n",
        "            latex += f\"{prefix}-VAE & {d} & {row['LL']} & {row['L[q]']} & {row['RE']} & {row['KL']} \\\\\\\\\\n\"\n",
        "\n",
        "        if d != LATENT_DIMS[-1]:\n",
        "            latex += r\"\\midrule\" + \"\\n\"\n",
        "\n",
        "    latex += r\"\"\"\n",
        "\\bottomrule\n",
        "\\end{tabular}\n",
        "\\end{table}\n",
        "\"\"\"\n",
        "    return latex\n",
        "\n",
        "latex_table = generate_latex_table(results_df)\n",
        "print(latex_table)\n",
        "\n",
        "with open(base_path + \"Table1_latex.tex\", 'w') as f:\n",
        "    f.write(latex_table)\n",
        "print(f\"\\nLaTeX saved to {base_path}Table1_latex.tex\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whsyoS19avmY"
      },
      "source": [
        "## **Quick Comparison with Paper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUXESzn5avmY"
      },
      "outputs": [],
      "source": [
        "paper_results = {\n",
        "    'N-VAE': {\n",
        "        2: {'LL': -135.73, 'ELBO': -137.08, 'RE': -129.84, 'KL': 7.24},\n",
        "        5: {'LL': -110.21, 'ELBO': -112.98, 'RE': -100.16, 'KL': 12.82},\n",
        "        10: {'LL': -93.84, 'ELBO': -98.36, 'RE': -78.93, 'KL': 19.44},\n",
        "        20: {'LL': -88.90, 'ELBO': -94.79, 'RE': -71.29, 'KL': 23.50},\n",
        "        40: {'LL': -88.93, 'ELBO': -94.91, 'RE': -71.14, 'KL': 23.77}\n",
        "    },\n",
        "    'S-VAE': {\n",
        "        2: {'LL': -132.50, 'ELBO': -133.72, 'RE': -126.43, 'KL': 7.28},\n",
        "        5: {'LL': -108.43, 'ELBO': -111.19, 'RE': -97.84, 'KL': 13.35},\n",
        "        10: {'LL': -93.16, 'ELBO': -97.70, 'RE': -77.03, 'KL': 20.67},\n",
        "        20: {'LL': -89.02, 'ELBO': -96.15, 'RE': -67.65, 'KL': 28.50},\n",
        "        40: {'LL': -90.87, 'ELBO': -101.26, 'RE': -67.75, 'KL': 33.50}\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Comparison: Our Results vs Paper\")\n",
        "print(\"=\"*80)\n",
        "for d in LATENT_DIMS:\n",
        "    for method in ['N-VAE', 'S-VAE']:\n",
        "        our = results_df[(results_df['Method'] == method) & (results_df['d'] == d)].iloc[0]\n",
        "        paper = paper_results[method][d]\n",
        "\n",
        "        print(f\"{method} d={d}:\")\n",
        "        print(f\"  LL:   Ours={our['LL_mean']:.2f} | Paper={paper['LL']:.2f} | Δ={our['LL_mean']-paper['LL']:.2f}\")\n",
        "        print(f\"  RE:   Ours={our['RE_mean']:.2f} | Paper={paper['RE']:.2f} | Δ={our['RE_mean']-paper['RE']:.2f}\")\n",
        "    print(\"-\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}