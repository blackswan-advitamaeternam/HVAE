{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba05c34",
   "metadata": {},
   "source": [
    "# **Reproducing Figure 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ad8eb",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/blackswan-advitamaeternam/HVAE/blob/raph/paper_experiments/Figure2_exp.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae68c0dc",
   "metadata": {},
   "source": [
    "## **Colab setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9652f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# to avoid having the data on your drive\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afe982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'HVAE'...\n",
      "remote: Enumerating objects: 354, done.\u001b[K\n",
      "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
      "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
      "remote: Total 354 (delta 6), reused 8 (delta 3), pack-reused 317 (from 1)\u001b[K\n",
      "Receiving objects: 100% (354/354), 276.30 KiB | 641.00 KiB/s, done.\n",
      "Resolving deltas: 100% (228/228), done.\n",
      "/content/HVAE\n",
      "Branch 'raph' set up to track remote branch 'raph' from 'origin'.\n",
      "Switched to a new branch 'raph'\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.16.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: umap-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (0.5.9.post2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (1.6.1)\n",
      "Collecting pathlib (from -r requirements.txt (line 8))\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (0.24.0+cu126)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (4.0.0)\n",
      "Collecting scanpy (from -r requirements.txt (line 13))\n",
      "  Downloading scanpy-1.11.5-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.5)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.12/dist-packages (from umap-learn->-r requirements.txt (line 6)) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.12/dist-packages (from umap-learn->-r requirements.txt (line 6)) (0.5.13)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 10)) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements.txt (line 12)) (6.0.3)\n",
      "Collecting anndata>=0.8 (from scanpy->-r requirements.txt (line 13))\n",
      "  Downloading anndata-0.12.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: h5py>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from scanpy->-r requirements.txt (line 13)) (3.15.1)\n",
      "Collecting legacy-api-wrap>=1.4.1 (from scanpy->-r requirements.txt (line 13))\n",
      "  Downloading legacy_api_wrap-1.5-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from scanpy->-r requirements.txt (line 13)) (8.4.0)\n",
      "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scanpy->-r requirements.txt (line 13)) (1.0.2)\n",
      "Collecting session-info2 (from scanpy->-r requirements.txt (line 13))\n",
      "  Downloading session_info2-0.3-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.12/dist-packages (from scanpy->-r requirements.txt (line 13)) (0.14.6)\n",
      "Collecting array-api-compat>=1.7.1 (from anndata>=0.8->scanpy->-r requirements.txt (line 13))\n",
      "  Downloading array_api_compat-1.13.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting zarr!=3.0.*,>=2.18.7 (from anndata>=0.8->scanpy->-r requirements.txt (line 13))\n",
      "  Downloading zarr-3.1.5-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets->-r requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.2->umap-learn->-r requirements.txt (line 6)) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 12)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 12)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 12)) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 10)) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 12)) (1.22.0)\n",
      "Collecting donfig>=0.8 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy->-r requirements.txt (line 13))\n",
      "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: google-crc32c>=1.5 in /usr/local/lib/python3.12/dist-packages (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy->-r requirements.txt (line 13)) (1.7.1)\n",
      "Collecting numcodecs>=0.14 (from zarr!=3.0.*,>=2.18.7->anndata>=0.8->scanpy->-r requirements.txt (line 13))\n",
      "  Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Downloading scanpy-1.11.5-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anndata-0.12.7-py3-none-any.whl (174 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.2/174.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading legacy_api_wrap-1.5-py3-none-any.whl (10 kB)\n",
      "Downloading session_info2-0.3-py3-none-any.whl (17 kB)\n",
      "Downloading array_api_compat-1.13.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zarr-3.1.5-py3-none-any.whl (284 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
      "Downloading numcodecs-0.16.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (9.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pathlib, session-info2, numcodecs, legacy-api-wrap, donfig, array-api-compat, zarr, anndata, scanpy\n",
      "Successfully installed anndata-0.12.7 array-api-compat-1.13.0 donfig-0.8.1.post1 legacy-api-wrap-1.5 numcodecs-0.16.5 pathlib-1.0.1 scanpy-1.11.5 session-info2-0.3 zarr-3.1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "25737d17920e4f62baccfe9175930100",
       "pip_warning": {
        "packages": [
         "pathlib"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!git clone https://github.com/blackswan-advitamaeternam/HVAE.git\n",
    "%cd HVAE\n",
    "!git checkout raph\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534b586",
   "metadata": {},
   "source": [
    "To allow automatic reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336ac563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (7.34.0)\n",
      "Collecting ipython\n",
      "  Downloading ipython-9.8.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /usr/local/lib/python3.12/dist-packages (from ipython) (4.4.2)\n",
      "Collecting ipython-pygments-lexers>=1.0.0 (from ipython)\n",
      "  Downloading ipython_pygments_lexers-1.1.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting jedi>=0.18.1 (from ipython)\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from ipython) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /usr/local/lib/python3.12/dist-packages (from ipython) (2.19.2)\n",
      "Collecting stack_data>=0.6.0 (from ipython)\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting traitlets>=5.13.0 (from ipython)\n",
      "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.18.1->ipython) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.14)\n",
      "Collecting executing>=1.2.0 (from stack_data>=0.6.0->ipython)\n",
      "  Downloading executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting asttokens>=2.1.0 (from stack_data>=0.6.0->ipython)\n",
      "  Downloading asttokens-3.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting pure-eval (from stack_data>=0.6.0->ipython)\n",
      "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading ipython-9.8.0-py3-none-any.whl (621 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m621.4/621.4 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading asttokens-3.0.1-py3-none-any.whl (27 kB)\n",
      "Downloading executing-2.2.1-py2.py3-none-any.whl (28 kB)\n",
      "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pure-eval, traitlets, jedi, ipython-pygments-lexers, executing, asttokens, stack_data, ipython\n",
      "  Attempting uninstall: traitlets\n",
      "    Found existing installation: traitlets 5.7.1\n",
      "    Uninstalling traitlets-5.7.1:\n",
      "      Successfully uninstalled traitlets-5.7.1\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 7.34.0\n",
      "    Uninstalling ipython-7.34.0:\n",
      "      Successfully uninstalled ipython-7.34.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 9.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed asttokens-3.0.1 executing-2.2.1 ipython-9.8.0 ipython-pygments-lexers-1.1.1 jedi-0.19.2 pure-eval-0.2.3 stack_data-0.6.3 traitlets-5.14.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "5c0acfcffffe4dd0a6a74779577434b1",
       "pip_warning": {
        "packages": [
         "IPython",
         "traitlets"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --upgrade ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29eaba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "try:\n",
    "    import imp\n",
    "except ImportError:\n",
    "    import types\n",
    "    sys.modules['imp'] = types.ModuleType('imp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaaceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c1a85",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d10f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "# To ensure the custom package is found\n",
    "path_to_repo = \"/content/HVAE\"\n",
    "if path_to_repo not in sys.path:\n",
    "    sys.path.append(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61b2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/HVAE/paper_experiments/load_MNIST.py\n",
      "/content/HVAE/paper_experiments\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from svae.vae import SVAE, GaussianVAE\n",
    "from svae.training import training\n",
    "from svae.utils import ShuffledLoader\n",
    "\n",
    "from paper_experiments.load_MNIST import make_splits_loaders_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dc020f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 workers.\n",
      "\n",
      "Making splits..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder /root/tensorflow_datasets/mnist/3.0.1 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/mnist/3.0.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a5037018124bdcb4ac2b1601dde48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3dbd20daf74427683b4c5d1cca7996b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7972b0acc664435795e4bb0cf2d5df7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2941800364749e4afcce5ee7e7179ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13077b147f724a3696fd37394317e096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e57722b4a14b2b9b2aa47b01125ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/mnist/incomplete.QR60UG_3.0.1/mnist-train.tfrecord*...:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c93ea55d1d4942a331fb7b844723ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca15556eb1c4f68a76b907db940f95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/mnist/incomplete.QR60UG_3.0.1/mnist-test.tfrecord*...:   0%|          | 0/…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\n",
      "600\n",
      "torch.Size([60000, 784])\n",
      "100\n",
      "torch.Size([10000, 784])\n",
      "Train 5000\n",
      "Val 1000\n",
      "Test 10000\n"
     ]
    }
   ],
   "source": [
    "# make splits\n",
    "NUM_WORKERS = int(0.8*os.cpu_count())\n",
    "FRAC = 1\n",
    "TRAIN_FRAC = int(50000 * FRAC)\n",
    "VAL_FRAC = int(10000 * FRAC)\n",
    "TEST_FRAC = None # potentially still test on all test samples\n",
    "print(f\"Using {NUM_WORKERS} workers.\")\n",
    "train_loader, val_loader, test_loader = make_splits_loaders_MNIST(train_size=TRAIN_FRAC, val_size=VAL_FRAC, test_size=TEST_FRAC,\n",
    "                                                                batch_size=100,\n",
    "                                                                test_batch_size=100,\n",
    "                                                                num_workers=NUM_WORKERS,\n",
    "                                                                prefetch_factor=2,\n",
    "                                                                force=True,\n",
    "                                                                persistent_workers=True,\n",
    "                                                                pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manifesting to memory of the device (we can afford it on colab ?)\n",
    "train_batches = [[el.to(DEVICE) for el in batch] for batch in train_loader]\n",
    "val_batches   = [[el.to(DEVICE) for el in batch] for batch in val_loader]\n",
    "test_batches  = [[el.to(DEVICE) for el in batch] for batch in test_loader]\n",
    "\n",
    "# Wrap for shuffling behavior\n",
    "train_loader = ShuffledLoader(\n",
    "    train_batches,\n",
    "    shuffle_batches=True,\n",
    "    shuffle_within_batch=True,\n",
    "    device_for_randperm=DEVICE)\n",
    "\n",
    "val_loader = val_batches\n",
    "test_loader = test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b8031",
   "metadata": {},
   "source": [
    "## **Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f06f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/content/drive/MyDrive/HVAE/Figure2/\"\n",
    "os.makedirs(base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "INPUT_DIM = 784\n",
    "HIDDEN_DIM = 128\n",
    "LATENT = 5\n",
    "\n",
    "LATENT_MODE = 'sample'\n",
    "PATIENCE = 50\n",
    "WARMUP = 100\n",
    "ONE_LAYER = False\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd492d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_latent_representations(model, loader, device):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # Depending on your specific Loader wrapper, batch might be a list or tuple.\n",
    "            # Assuming standard structure [data, labels]\n",
    "            x = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "            \n",
    "            # Access latent samples as specified\n",
    "            # svae_latent_samples, _, _ = model.get_latent_samples(data_tensor)\n",
    "            z, _, _ = model.get_latent_samples(x)\n",
    "            \n",
    "            latents.append(z.cpu().numpy())\n",
    "            labels.append(y.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(latents), np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hammer_projection_S2(latents, labels, save_path):\n",
    "    \"\"\"\n",
    "    Plots the S2 latent space using the Hammer projection to resemble Figure 2b.\n",
    "    \"\"\"\n",
    "    # 1. Normalize vectors (Project onto unit sphere)\n",
    "    norms = np.linalg.norm(latents, axis=1, keepdims=True)\n",
    "    latents = latents / (norms + 1e-8)\n",
    "    \n",
    "    x_coord, y_coord, z_coord = latents[:, 0], latents[:, 1], latents[:, 2]\n",
    "    \n",
    "    # 2. Convert Cartesian to Spherical coordinates for Hammer projection\n",
    "    # Longitude (lambda): [-pi, pi], Latitude (phi): [-pi/2, pi/2]\n",
    "    longitude = np.arctan2(y_coord, x_coord) \n",
    "    latitude = np.arcsin(z_coord) \n",
    "    \n",
    "    # 3. Setup Plot\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111, projection='hammer')\n",
    "    \n",
    "    # Colors for the 10 MNIST digits\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    \n",
    "    # Scatter plot per digit\n",
    "    for i in range(10):\n",
    "        idx = (labels == i)\n",
    "        if np.sum(idx) > 0:\n",
    "            ax.scatter(longitude[idx], latitude[idx], \n",
    "                       label=str(i), \n",
    "                       s=8,           # Small point size like the paper\n",
    "                       alpha=0.7,     # Slight transparency\n",
    "                       color=colors[i],\n",
    "                       edgecolors='none')\n",
    "            \n",
    "    # 4. Remove Axes and Grids to match Figure 2 style\n",
    "    ax.grid(False)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    # We keep the spinal border to show the Hammer oval shape, \n",
    "    # but remove the ticks.\n",
    "    ax.tick_params(axis='both', which='both', length=0)\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"Digit\", frameon=False)\n",
    "    plt.title(\"S-VAE Latent Space ($S^2$ Hammer Projection)\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_R2(latents, labels, save_path):\n",
    "    \"\"\"\n",
    "    Plots the R2 latent space to resemble Figure 2a.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    \n",
    "    for i in range(10):\n",
    "        idx = (labels == i)\n",
    "        if np.sum(idx) > 0:\n",
    "            plt.scatter(latents[idx, 0], latents[idx, 1], \n",
    "                        label=str(i), \n",
    "                        s=8, \n",
    "                        alpha=0.7, \n",
    "                        color=colors[i],\n",
    "                        edgecolors='none')\n",
    "            \n",
    "    # Remove all axes for the clean \"cloud\" look seen in Figure 2a\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.legend(title=\"Digit\", frameon=False)\n",
    "    plt.title(\"N-VAE Latent Space ($\\mathbb{R}^2$)\")\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346044cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n[SVAE] Instantiating SVAE and optimizer..\")\n",
    "model = SVAE(INPUT_DIM,\n",
    "            HIDDEN_DIM,\n",
    "            3, # S^2\n",
    "            ONE_LAYER,\n",
    "            mode='MNIST')\n",
    "# To device\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(f\"[SVAE] Started training..\")\n",
    "model, losses, all_parts = training(train_loader,\n",
    "                                val_loader,\n",
    "                                model,\n",
    "                                optimizer,\n",
    "                                epochs=EPOCHS,\n",
    "                                beta_kl=1,\n",
    "                                warmup=WARMUP,\n",
    "                                patience=PATIENCE,\n",
    "                                show_loss_every=1,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fc383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting SVAE representations...\")\n",
    "# Ideally, pass the SVAE model object here\n",
    "z_svae, y_svae = get_latent_representations(model, test_loader, DEVICE)\n",
    "\n",
    "svae_save_path = os.path.join(base_path, \"figure2_svae_hammer.pdf\")\n",
    "plot_hammer_projection_S2(z_svae, y_svae, svae_save_path)\n",
    "print(f\"SVAE plot saved to {svae_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n[NVAE] Instantiating NVAE and optimizer..\")\n",
    "model = GaussianVAE(INPUT_DIM,\n",
    "            HIDDEN_DIM,\n",
    "            2, # R^2\n",
    "            ONE_LAYER,\n",
    "            mode='MNIST')\n",
    "# To device\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(f\"[NVAE] Started training..\")\n",
    "model, losses, all_parts = training(train_loader,\n",
    "                                val_loader,\n",
    "                                model,\n",
    "                                optimizer,\n",
    "                                epochs=EPOCHS,\n",
    "                                beta_kl=1,\n",
    "                                warmup=WARMUP,\n",
    "                                patience=PATIENCE,\n",
    "                                show_loss_every=1,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting NVAE representations...\")\n",
    "z_nvae, y_nvae = get_latent_representations(model, test_loader, DEVICE)\n",
    "\n",
    "nvae_save_path = os.path.join(base_path, \"figure2_nvae_R2.pdf\")\n",
    "plot_R2(z_nvae, y_nvae, nvae_save_path)\n",
    "print(f\"NVAE plot saved to {nvae_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
